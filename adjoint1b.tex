\documentclass{beamer}
\usetheme{Boadilla}

\title{Markov Categories and Entropy}
\subtitle{Paolo Perrone, IEEE Transactions on Information Theory, 2023}

\author{Utku BoduroÄŸlu, Drew McNeely, and Nico Wittrock}
\institute{Adjoint School 2024}
\date{April 11, 2024}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\section{Introduction to Markov Categories}
\input{part1/part1}
\section{Useful ways to construct Markov categories}
\input{part2/part2}
\section{Properties and additional axioms}
\input{part3/part3}

\begin{frame}
\frametitle{Outline}
This is just me fooling around with Beamer to get a feel for it.
None of the contents should be taken seriously at this point.
\tableofcontents
\end{frame}

\section{Markov Categories}
\subsection{Axioms}

\begin{frame}
\frametitle{Definition of Markov Category}
\begin{definition}
	A Markov category is a semi-Cartesian category in which every object is a commutative comonoid.
\end{definition}
\end{frame}

\subsection{Examples}

\begin{frame}
	\frametitle{Example Markov category}
	Okay if we create an example, it absolutely needs to involve the cloud cover forecast during the total solar eclipse.
\end{frame}

\section{Enriched Categories}

\begin{frame}
	\frametitle{Enriched Categories}
	So yeah an enriched category is like a category with more structure on the morphisms.
	Instead of having hom-sets, ya got yourself some hom-objects.
\end{frame}

\section{Divergences}

\begin{frame}
	\frametitle{Divergence}
	Aight, so a divergence is like a metric for probability distributions...

	Except it ain't a metric, cause it ain't \emph{sym}metric.
\end{frame}

\subsection{Entropy}

\begin{frame}
	\frametitle{Entropy}
	Entropy is a measure of how ``random'' a probability distribution is, given through the divergence between both sides of the defining equation for deterministic kernels. Neat!!
\end{frame}

\end{document}
